{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP522Y5y12nDHKzrAuPp/X/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jandsy/ml_finance_imperial/blob/main/Additional_Materials/Optional_Reading_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias-Variance Tradeoff Analysis\n",
        "\n",
        "## General Model Framework\n",
        "\n",
        "Consider a prediction model where the output $Y$ is related to inputs $X$ through a true function $f$, with added noise $\\epsilon$:\n",
        "$$\n",
        "Y = f(X) + \\epsilon\n",
        "$$\n",
        "where $\\epsilon$ is a noise term with $E[\\epsilon] = 0$ and $\\text{Var}(\\epsilon) = \\sigma^2$, independent of $X$.\n",
        "\n",
        "Let $f_D$ be the model trained on dataset $D$. The prediction for a new input $X = x$ is given by $f_D(x)$.\n",
        "\n",
        "### Expected Prediction Error (EPE)\n",
        "\n",
        "The EPE at a point $x$ for predictions made by $f_D$ is:\n",
        "$$\n",
        "E[(Y - f_D(x))^2 | X = x]\n",
        "$$\n",
        "Expanding this, we find:\n",
        "$$\n",
        "E[(f(x) + \\epsilon - f_D(x))^2 | X = x] = E[(f(x) - f_D(x) + \\epsilon)^2 | X = x]\n",
        "$$\n",
        "$$\n",
        "= E[(f(x) - f_D(x))^2 | X = x] + 2E[\\epsilon(f(x) - f_D(x)) | X = x] + E[\\epsilon^2 | X = x]\n",
        "$$\n",
        "Given $\\epsilon$ is independent of $X$ and has mean 0:\n",
        "$$\n",
        "E[\\epsilon(f(x) - f_D(x)) | X = x] = 0\n",
        "$$\n",
        "The EPE then simplifies to:\n",
        "$$\n",
        "EPE = \\text{Bias}^2(f_D(x)) + \\text{Var}(f_D(x)) + \\sigma^2\n",
        "$$\n",
        "\n",
        "#### Bias and Variance Definitions\n",
        "\n",
        "- **Bias** of $f_D$ at $x$:\n",
        "  $$\n",
        "  \\text{Bias}(f_D(x)) = E[f_D(x) | X = x] - f(x)\n",
        "  $$\n",
        "  Squared bias:\n",
        "  $$\n",
        "  \\text{Bias}^2(f_D(x)) = (E[f_D(x) | X = x] - f(x))^2\n",
        "  $$\n",
        "\n",
        "- **Variance** of $f_D$ at $x$:\n",
        "  $$\n",
        "  \\text{Var}(f_D(x)) = E[(f_D(x) - E[f_D(x) | X = x])^2 | X = x]\n",
        "  $$\n",
        "\n",
        "### Impact of Increasing Dataset Size\n",
        "\n",
        "Increasing the dataset size generally results in:\n",
        "- **Reduced Variance:** $\\text{Var}(f_D(x))$ decreases due to the law of large numbers, assuming the model is well-posed.\n",
        "- **Unchanged Bias:** Bias does not change unless the functional form of $f_D$ is altered. It is determined by the modelâ€™s capacity to approximate $f$.\n",
        "\n",
        "## Special Case: Linear Regression\n",
        "\n",
        "In the context of linear regression, consider the model:\n",
        "$$\n",
        "Y = X\\beta + \\epsilon\n",
        "$$\n",
        "where $X$ is the matrix of input features, $\\beta$ is the vector of coefficients, and $\\epsilon \\sim N(0, \\sigma^2I)$.\n",
        "\n",
        "### OLS Estimator\n",
        "\n",
        "The ordinary least squares (OLS) estimator for $\\beta$ is:\n",
        "$$\n",
        "\\hat{\\beta} = (X^T X)^{-1} X^T Y\n",
        "$$\n",
        "The prediction at a new point $X = x_0$ is:\n",
        "$$\n",
        "\\hat{Y}_0 = x_0^T \\hat{\\beta}\n",
        "$$\n",
        "\n",
        "### Bias and Variance in Linear Regression\n",
        "\n",
        "- **Bias**:\n",
        "  Assuming the model form correctly includes all relevant variables and interactions:\n",
        "  $$\n",
        "  \\text{Bias}(\\hat{Y}_0) = 0\n",
        "  $$\n",
        "\n",
        "- **Variance**:\n",
        "  The variance of predictions is influenced by the inverse of the design matrix:\n",
        "  $$\n",
        "  \\text{Var}(\\hat{Y}_0) = \\sigma^2 x_0^T (X^T X)^{-1} x_0\n",
        "  $$\n",
        "\n",
        "### Impact of Dataset Size in Linear Regression\n",
        "\n",
        "Increasing the dataset size:\n",
        "- **Reduces Variance:** The matrix $(X^T X)$ becomes larger, making $(X^T X)^{-1}$ smaller, thus reducing the variance of the estimator $\\hat{\\beta}$ and predictions $\\hat{Y}_0$.\n",
        "- **Bias Unchanged:** The bias remains zero if the model is correctly specified.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The bias-variance tradeoff illustrates the fundamental challenges in model training. For both general and linear models, increasing the dataset size improves model accuracy by reducing variance without affecting bias, provided the model's complexity is appropriate for the underlying function $f$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oiiYTIZmMgFb"
      }
    }
  ]
}